<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Blind Assistance Object Detection</title>
  <style>
    body {
      margin: 0;
      font-family: sans-serif;
      background: #0f2027;
      color: white;
      display: flex;
      flex-direction: column;
      align-items: center;
    }
    h2 { margin-top: 20px; }
    #video, canvas {
      margin-top: 10px;
      width: 90%;
      max-width: 600px;
      border: 3px solid white;
      border-radius: 10px;
    }
    #controls { margin-top: 15px; }
    button {
      padding: 10px 20px;
      margin: 10px;
      font-size: 16px;
      font-weight: bold;
      border: none;
      border-radius: 6px;
      cursor: pointer;
    }
    #startBtn { background-color: #00c853; color: white; }
    #stopBtn { background-color: #d50000; color: white; }

    /* üìù Speech log box */
    #speechLog {
      margin-top: 20px;
      width: 90%;
      max-width: 600px;
      background: rgba(255,255,255,0.1);
      padding: 10px;
      border-radius: 6px;
      font-size: 16px;
      min-height: 40px;
      border: 1px solid #ccc;
      text-align: center;
    }
  </style>
</head>
<body>

  <h2>Blind Assist - Object Detection</h2>
  <div id="controls">
    <button id="startBtn">Start Detection</button>
    <button id="stopBtn">Stop Detection</button>
  </div>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>

  <!-- üìù Live speech log -->
  <div id="speechLog">üé§ Waiting for voice command...</div>

  <!-- TensorFlow.js and COCO-SSD -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d');
    const speechLog = document.getElementById('speechLog');
    let model, stream, detectionInterval;

    // üîä Speak helper
    const speak = (msg) => {
      const utterance = new SpeechSynthesisUtterance(msg);
      window.speechSynthesis.cancel();
      window.speechSynthesis.speak(utterance);
    };

    // üì≥ Vibration
    const vibrate = () => {
      if ('vibrate' in navigator) {
        navigator.vibrate([200, 100, 200]);
      }
    };

    // üé• Camera
    async function startCamera() {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        return new Promise((resolve) => {
          video.onloadedmetadata = () => resolve();
        });
      } catch (err) {
        alert("Camera access denied: " + err.message);
      }
    }

    async function loadModel() {
      model = await cocoSsd.load();
    }

    // üëÅ Object Detection
    async function detectObjects() {
      if (!model) return;
      const predictions = await model.detect(video);
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      if (predictions.length === 0) {
        speak("Unknown object detected");
        return;
      }

      predictions.forEach(pred => {
        const [x, y, width, height] = pred.bbox;
        ctx.strokeStyle = "#00ff00";
        ctx.lineWidth = 3;
        ctx.strokeRect(x, y, width, height);
        ctx.fillStyle = "#00ff00";
        ctx.font = "18px Arial";
        ctx.fillText(pred.class, x, y - 5);

        if (pred.score > 0.7) {
          speak(pred.class);
          if (width > canvas.width * 0.5) {
            vibrate();
          }
        }
      });
    }

    async function startDetection() {
      await startCamera();
      await loadModel();
      detectionInterval = setInterval(detectObjects, 1500);
      speak("Detection started");
    }

    function stopDetection() {
      clearInterval(detectionInterval);
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
        video.srcObject = null;
        ctx.clearRect(0, 0, canvas.width, canvas.height);
      }
      speak("Detection stopped");
    }

    // üñ± Backup buttons
    document.getElementById('startBtn').addEventListener('click', startDetection);
    document.getElementById('stopBtn').addEventListener('click', stopDetection);

    // üé§ Voice control
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;   // keep listening
      recognition.interimResults = false;
      recognition.lang = "en-IN"; // Change to en-US if needed

      recognition.onresult = (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript.trim().toLowerCase();
        console.log("Heard:", transcript);

        // üìù Show transcript in log
        speechLog.textContent = "Heard: " + transcript;

        if (transcript.includes("start")) {
          startDetection();
        } else if (transcript.includes("stop")) {
          stopDetection();
        }
      };

      recognition.onerror = (e) => {
        console.warn("Speech error:", e.error);
        speechLog.textContent = "Error: " + e.error;
        if (e.error === "no-speech" || e.error === "network") {
          recognition.stop();
          setTimeout(() => recognition.start(), 1000); // auto-restart
        }
      };

      recognition.onend = () => {
        recognition.start(); // restart if stopped
      };

      recognition.start();
      speak("Voice control ready. Say start to begin detection or stop to end.");
    } else {
      speak("Speech recognition not supported in this browser");
      speechLog.textContent = "‚ùå Speech recognition not supported in this browser";
    }
  </script>
</body>
</html>
